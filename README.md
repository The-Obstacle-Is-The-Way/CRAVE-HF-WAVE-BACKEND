# ğŸš€ CRAVE Trinity Backend (FastAPI + Clean Architecture)  

## ğŸŒŸ Overview  

CRAVE Trinity Backend is a **modular, Dockerized FastAPI application** built with **clean architecture** principles. It is designed to **track and analyze user cravings** and integrates multiple external services, including:  

- ğŸ›¢ **PostgreSQL** for structured data storage  
- ğŸ§  **Pinecone** for vector-based retrieval (Batch 3)  
- ğŸ¤– **Llama 2 with LoRA integration** for AI-powered insights (Batch 4)  

This repository demonstrates an **end-to-end system**â€”from **initial setup** and **database migrations** to **AI model inference with LoRA adapters**.  

---

## ğŸ— Architecture & Batches  

The project is organized into **batches**, breaking the development process into structured steps:  

### ğŸ”¹ Batch 1 â€“ Initial Setup  
ğŸ“Œ Clone the repository, install dependencies, and configure the environment.  
ğŸ”§ Initialize **PostgreSQL** and apply **Alembic** database migrations.  
ğŸ“‚ **Key files:** `.env`, `requirements.txt`, `alembic.ini`  

### ğŸ”¹ Batch 2 â€“ Backend & Database Integration  
ğŸ›  Develop **FastAPI** REST endpoints following **clean architecture**.  
ğŸ“Š Implement **database models, repositories, and use-case layers** for craving tracking.  
ğŸ“‚ **Key files:** `app/api/`, `app/core/`, `app/infrastructure/database/`  

### ğŸ”¹ Batch 3 â€“ External Services Integration  
ğŸ“¡ Connect to **Pinecone** for **vector storage & retrieval**.  
ğŸ¤– Integrate **OpenAI embeddings** for craving analysis.  
ğŸ“‚ **Key files:** `app/infrastructure/vector_db/`, `app/infrastructure/external/openai_embedding.py`  

### ğŸ”¹ Batch 4 â€“ Llama 2 with LoRA Integration  
ğŸ¦™ Load and fine-tune **Llama 2** using **LoRA adapters**.  
ğŸ” Deploy AI inference endpoints for **craving insights**.  
ğŸ“‚ **Key files:** `app/models/llama2_model.py`, `app/infrastructure/llm/llama2_adapter.py`  

---

## âš¡ Quick Start  

### âœ… Prerequisites  

Before you begin, ensure you have the following installed:  

- ğŸ³ **Docker & Docker Compose** for containerized setup  
- ğŸ **Python 3.11** (if running locally)  
- ğŸ¤— **Hugging Face CLI** (if using private models, run `huggingface-cli login`)  

### ğŸ“¥ Clone the Repository  

```bash
git clone git@github.com:The-Obstacle-Is-The-Way/crave-trinity-backend.git
cd crave-trinity-backend
```

### ğŸ”§ Configure Environment Variables  

Create a `.env` file in the project root with the necessary credentials:  

```env
SQLALCHEMY_DATABASE_URI=postgresql://postgres:password@db:5432/crave_db
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENV=us-east-1-aws
PINECONE_INDEX_NAME=crave-embeddings
OPENAI_API_KEY=your_openai_api_key_here
```

### ğŸ— Build & Run with Docker Compose  

```bash
docker-compose up --build
```

This will:  
âœ… Build the **FastAPI** backend container  
âœ… Start the **PostgreSQL** database  
âœ… Expose ports **8000** (API) & **5432** (Database)  

### ğŸ”„ Run Database Migrations  

Inside the container (or locally, if configured):  

```bash
alembic upgrade head
```

This ensures the **database schema** is up to date.  

---

## ğŸ§ª Testing the Application  

### ğŸ”¬ API Endpoints  

Once running, test the **craving logging API** with:  

```bash
curl -X POST -H "Content-Type: application/json" \
-d '{"user_id":1, "description":"Chocolate craving", "intensity":8}' \
http://localhost:8000/cravings
```

### ğŸ“¡ Pinecone Integration  

Inside the **FastAPI** container, verify the Pinecone index:  

```bash
docker exec -it crave_trinity_backend-fast-api-1 python -c \
"from app.infrastructure.vector_db.pinecone_client import init_pinecone; \
init_pinecone(); import pinecone; print('List of indexes:', pinecone.list_indexes())"
```

Ensure `crave-embeddings` exists and is ready for use.  

### ğŸ¤– Run Llama 2 with LoRA Inference (Batch 4)  

```bash
docker exec -it crave_trinity_backend-fast-api-1 python app/models/llama2_model.py
```

This loads **Llama 2 + LoRA adapters** and runs a **test inference prompt**.  

---

## ğŸ›  Technical Details  

- ğŸ³ **Dockerized Setup**  
  - The backend is containerized with **Python 3.11-slim** for efficiency.  

- ğŸ›¢ **Database**  
  - Uses **PostgreSQL**, managed via **Alembic** migrations.  

- ğŸ“¡ **External Services**  
  - **Pinecone** for **vector storage & retrieval**.  
  - **OpenAI** for **text embeddings** and craving analysis.  

- ğŸ¤– **AI Model (Batch 4)**  
  - **Llama 2** runs via **Hugging Face Transformers**.  
  - **LoRA adapters** fine-tune AI insights with **PEFT**.  

---

## ğŸ›£ Roadmap & Future Enhancements  

ğŸ”œ **Batch 5** â€“ Analytics dashboard & craving trend visualization  
ğŸ“Š **Batch 6** â€“ Performance optimizations (GPU inference, rate limiting)  
ğŸ”’ **Security Enhancements** â€“ OAuth, data anonymization, and logging improvements  
ğŸš€ **Scaling** â€“ Kubernetes deployment (`infra/k8s`)  

---

## ğŸ“‚ File Structure  

```plaintext
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/           # API endpoints
â”‚   â”œâ”€â”€ config/        # Settings & logging
â”‚   â”œâ”€â”€ core/          # Business logic (use cases)
â”‚   â”œâ”€â”€ infrastructure/# Database, external APIs, vector DBs
â”‚   â”œâ”€â”€ models/        # AI model handling (Llama2 + LoRA)
â”œâ”€â”€ infra/             # Deployment configs (AWS, K8s, Docker)
â”œâ”€â”€ main.py            # Application entry point
â”œâ”€â”€ requirements.txt   # Dependencies
â””â”€â”€ tests/             # Unit & integration tests
```

---

## ğŸ¤ Contributing  

1ï¸âƒ£ **Fork** the repository  
2ï¸âƒ£ **Create** a feature branch (`git checkout -b feature/your-feature`)  
3ï¸âƒ£ **Commit** your changes (`git commit -m "Added feature X"`)  
4ï¸âƒ£ **Push** & open a pull request  

---

## ğŸ“œ License  

This project is licensed under the **MIT License**.  
